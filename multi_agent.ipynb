{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVnihFEuL3eEVj04Igs8xF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aravind-11/IITM_Saastra/blob/main/multi_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTNI5rrhCis6"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "class Football:  # The class encapsulating the environment\n",
        "    '''\n",
        "    Actions [0 : Stand, 1 : Up, 2 : Right, 3 : Down, 4 : Left]\n",
        "    These are the representing no.s for the mentioned actions\n",
        "    '''\n",
        "\n",
        "    def __init__(self, length=8, width=8, goalPositions=[4, 4]):\n",
        "        \n",
        "        # The player start at random locations\n",
        "        \n",
        "        self.pA=np.array([np.random.randint(length), np.random.randint(length)]) \n",
        "        self.pB=np.array([np.random.randint(length), np.random.randint(length)]) \n",
        "        \n",
        "        self.h = length   # Length of the Football Pitch    \n",
        "        self.w = width    # Width of the Football Pitch\n",
        "        \n",
        "        self.goalPositions = np.array(goalPositions)   # This means that the middle 4 positions at the right and left are the goals\n",
        "        \n",
        "        self.reward = np.array([0,0])\n",
        "        \n",
        "                                  # Initially the reward is 0\n",
        "        \n",
        "        self.observation=np.random.rand(5,)\n",
        "        self.done = bool(0)    \n",
        "        self.observation_space=gym.spaces.Box(low=-8, high=8,\n",
        "                                        shape=(5,), dtype=np.float32)\n",
        "        self.ballOwner = np.random.randint(2)\n",
        "        self.action_space=gym.spaces.Discrete(5)\n",
        "    \n",
        "\n",
        "    def isInBoard(self, x, y):\n",
        "        if(x<0 or x>8):\n",
        "          return 0\n",
        "        if(y<0 or y>8):\n",
        "          return 0 \n",
        "        return 1\n",
        "    \n",
        "    def actionToMove(self, action):\n",
        "        switcher = {\n",
        "            0: [0, 0],\n",
        "            1: [0, 1],\n",
        "            2: [1, 0],\n",
        "            3: [0, -1],\n",
        "            4: [-1, 0],\n",
        "        }\n",
        "        return switcher.get(action)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAhlYbruCrtw"
      },
      "source": [
        "class Agent_A(Football,gym.Env):\n",
        "  def __init__(self, length=8, width=8, goalPositions=[2, 6]):\n",
        "    super().__init__()\n",
        "    \n",
        "    \n",
        "  def reset(self):\n",
        "        self.done = bool(0)\n",
        "        \n",
        "        self.pA = np.array([np.random.randint(self.h), np.random.randint(self.h)])\n",
        "        self.pB = np.array([np.random.randint(self.h), np.random.randint(self.h)])\n",
        "        return np.array((*self.pA,*self.pB,(4-self.pA[0]),(4-self.pA[1]))).astype(np.float32)\n",
        "  def step(self, action):\n",
        "        if self.done == 1:\n",
        "          self.reset()\n",
        "        self.move(action)                   # We chose the first player at random !!! important thing to consider - how to choose first player . \n",
        "        if self.done == 1:\n",
        "          return self.observation, self.reward[0], self.done\n",
        "\n",
        "        return self.observation, self.reward[0].astype(float), self.done,{}\n",
        "  \n",
        "  def move(self, action):\n",
        "        \n",
        "        newPosition = self.pA + self.actionToMove(action)\n",
        "\n",
        "        # If it's opponent position\n",
        "        if (newPosition == self.pB).any():\n",
        "            self.ballOwner = 1\n",
        "            self.reward[0]=-20\n",
        "            self.reward[1]=20\n",
        "        # If it's a goal\n",
        "        elif self.ballOwner is 0 and self.isInGoal(*newPosition) >= 0:\n",
        "            self.done = 1\n",
        "            return 1 - self.isInGoal(*newPosition)\n",
        "        # If it's in the board\n",
        "        elif self.isInBoard(*newPosition):\n",
        "            self.reward[0] =  -0.1 * ((((abs(newPosition[0]-self.pB[0])+0.1)+(abs(newPosition[1]-self.pB[1])+0.1))) -  (self.ballOwner) )\n",
        "            self.pA = newPosition\n",
        "        self.observation=np.array((*self.pA,*self.pB,self.ballOwner)).astype(np.float32)\n",
        "        return -1\n",
        "  def reset(self):\n",
        "        self.done = bool(0)\n",
        "        self.reward = np.array([0,0])\n",
        "        \n",
        "        self.pA = np.array([np.random.randint(self.h), np.random.randint(self.h)])\n",
        "        self.pB = np.array([np.random.randint(self.h), np.random.randint(self.h)]) \n",
        "        return np.array((*self.pA,*self.pB,self.ballOwner)).astype(np.float32)\n",
        "  def render(self,mode='console'):\n",
        "        board = ''\n",
        "        for y in range(self.h)[::-1]:\n",
        "            for x in range(self.w):\n",
        "                if ([x, y] == self.pA).all():\n",
        "                    board += 'A' if self.ballOwner is 0 else 'a'\n",
        "                elif ([x, y] == self.pB).all():\n",
        "                    board += 'B' if self.ballOwner is 1 else 'b'\n",
        "                else:\n",
        "                    board += '-'\n",
        "            board += '\\n'\n",
        "\n",
        "  def isInGoal(self, x, y):\n",
        "        g1, g2 = self.goalPositions\n",
        "        if (g1 <= y <= g2):\n",
        "            if x == 0:\n",
        "                self.done = bool(1)\n",
        "                self.reward[0] = -20 # if the ball reaches the right goal post, then the rewards shall be -1\n",
        "                return 1 \n",
        "            elif x == (self.w-1):\n",
        "                self.done = bool(1)\n",
        "                self.reward[0] = 20 # if the ball reaches the right goal post, then the rewards shall be 1\n",
        "                return 0\n",
        "        return -1\n",
        "\n",
        "  def seed():\n",
        "      return 0 \n",
        "  def metadata(x):\n",
        "      return 0 \n",
        "  def legal_actions(self):\n",
        "    return gym.spaces.Discrete(5)\n",
        "  def close(self):\n",
        "    pass"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPBAePCiWDT_"
      },
      "source": [
        "env1=Agent_A(Football,gym.Env)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qML9AfUWDyA1"
      },
      "source": [
        "class Agent_B(Football,gym.Env):\n",
        "  def __init__(self, length=8, width=8, goalPositions=[2, 6]):\n",
        "    super().__init__()\n",
        "    \n",
        "    \n",
        "  def reset(self):\n",
        "        self.done = bool(0)\n",
        "        \n",
        "        self.pA = np.array([np.random.randint(self.h), np.random.randint(self.h)])\n",
        "        self.pB = np.array([np.random.randint(self.h), np.random.randint(self.h)])\n",
        "        return np.array((*self.pB,*self.pA,self.ballOwner)).astype(np.float32)\n",
        "  def step(self, action):\n",
        "        if self.done == 1:\n",
        "          self.reset()\n",
        "        self.move(action)                   # We chose the first player at random !!! important thing to consider - how to choose first player . \n",
        "        if self.done == 1:\n",
        "          return self.observation, self.reward[1], self.done\n",
        "\n",
        "        return self.observation, self.reward[1].astype(np.float), self.done,{}\n",
        "  \n",
        "  def move(self, action):\n",
        "        \n",
        "        newPosition = self.pB + self.actionToMove(action)\n",
        "\n",
        "        # If it's opponent position\n",
        "        if (newPosition == self.pA).any():\n",
        "            self.ballOwner = 0\n",
        "            self.reward[1]=-20\n",
        "            self.reward[0]=20\n",
        "        # If it's a goal\n",
        "        elif self.ballOwner is 1 and self.isInGoal(*newPosition) >= 0:\n",
        "            self.done = 1\n",
        "            return 1 - self.isInGoal(*newPosition)\n",
        "        # If it's in the board\n",
        "        elif self.isInBoard(*newPosition):\n",
        "            self.reward[1] =  -0.1 * ((((abs(newPosition[0]-self.pA[0])+0.1)+(abs(newPosition[1]-self.pA[1])+0.1))) + (self.ballOwner) )\n",
        "            self.pB = newPosition\n",
        "        self.observation=np.array((*self.pB,*self.pA,self.ballOwner)).astype(np.float32)\n",
        "        return -1\n",
        "  def reset(self):\n",
        "        self.done = bool(0)\n",
        "        self.reward = np.array([0,0])\n",
        "\n",
        "        self.pA = np.array([np.random.randint(self.h), np.random.randint(self.h)])\n",
        "        self.pB = np.array([np.random.randint(self.h), np.random.randint(self.h)]) \n",
        "        return np.array((*self.pB,*self.pA,self.ballOwner)).astype(np.float32)\n",
        "  def render(self,mode='console'):\n",
        "        board = ''\n",
        "        for y in range(self.h)[::-1]:\n",
        "            for x in range(self.w):\n",
        "                if ([x, y] == self.pA).all():\n",
        "                    board += 'A' if ballOwner is 0 else 'a'\n",
        "                elif ([x, y] == self.pB).all():\n",
        "                    board += 'B' if ballOwner is 1 else 'b'\n",
        "                else:\n",
        "                    board += '-'\n",
        "            board += '\\n'\n",
        "  def isInGoal(self, x, y):\n",
        "        g1, g2 = self.goalPositions\n",
        "        if (g1 <= y <= g2):\n",
        "            if x == 0:\n",
        "                self.done = bool(1)\n",
        "                self.reward[1] = -20 # if the ball reaches the right goal post, then the rewards shall be -1\n",
        "                return 1 \n",
        "            elif x == (self.w-1):\n",
        "                self.done = bool(1)\n",
        "                self.reward[1] = 20 # if the ball reaches the right goal post, then the rewards shall be 1\n",
        "                return 0\n",
        "        return -1\n",
        "  def seed():\n",
        "      return 0 \n",
        "  def metadata(x):\n",
        "      return 0 \n",
        "  def legal_actions(self):\n",
        "    return gym.spaces.Discrete(5)\n",
        "  def close(self):\n",
        "    pass"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAVZUc4H0_uK"
      },
      "source": [
        "env2=Agent_B(Football,gym.Env)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twZOQXF60_KR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccea5d88-dce2-4015-aa60-8958cad5c74f"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "  Downloading stable_baselines-2.10.0-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Installing collected packages: stable-baselines\n",
            "  Attempting uninstall: stable-baselines\n",
            "    Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCKuQTfQ1IRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4cdc98-87e3-44d2-c144-56f2a1c19d44"
      },
      "source": [
        "from stable_baselines.common.env_checker import check_env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xnffJrv1K03"
      },
      "source": [
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHUcyqRd1K2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9d929e-e098-414b-afca-3cb11717e860"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "#env = GoLeftEnv(grid_size=10)\n",
        "# wrap it\n",
        "env1 = make_vec_env(lambda: env1, n_envs=1)\n",
        "env2 = make_vec_env(lambda: env2, n_envs=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j1rooua2biy",
        "outputId": "34c94fd9-1baa-42aa-d594-3823ab20485b"
      },
      "source": [
        "# Train the agent\n",
        "\n",
        "model1 = PPO2('MlpPolicy', env1, verbose=1).learn(5000)\n",
        "model2 = PPO2('MlpPolicy', env2, verbose=1).learn(5000)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapping the env in a DummyVecEnv.\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003761745  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00265      |\n",
            "| fps                | 348           |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 1.6090072     |\n",
            "| policy_loss        | -0.0042407834 |\n",
            "| serial_timesteps   | 128           |\n",
            "| time_elapsed       | 1.6e-05       |\n",
            "| total_timesteps    | 128           |\n",
            "| value_loss         | 232.13898     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.00045641   |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00419     |\n",
            "| fps                | 851          |\n",
            "| n_updates          | 2            |\n",
            "| policy_entropy     | 1.6057125    |\n",
            "| policy_loss        | -0.006642286 |\n",
            "| serial_timesteps   | 256          |\n",
            "| time_elapsed       | 0.368        |\n",
            "| total_timesteps    | 256          |\n",
            "| value_loss         | 543.44415    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0010578329 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00193     |\n",
            "| fps                | 1017         |\n",
            "| n_updates          | 3            |\n",
            "| policy_entropy     | 1.5978028    |\n",
            "| policy_loss        | -0.014881021 |\n",
            "| serial_timesteps   | 384          |\n",
            "| time_elapsed       | 0.521        |\n",
            "| total_timesteps    | 384          |\n",
            "| value_loss         | 1202.852     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0018939862 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00497     |\n",
            "| fps                | 1011         |\n",
            "| n_updates          | 4            |\n",
            "| policy_entropy     | 1.5764743    |\n",
            "| policy_loss        | -0.008989834 |\n",
            "| serial_timesteps   | 512          |\n",
            "| time_elapsed       | 0.649        |\n",
            "| total_timesteps    | 512          |\n",
            "| value_loss         | 848.09094    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00096272345 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.0079       |\n",
            "| fps                | 914           |\n",
            "| n_updates          | 5             |\n",
            "| policy_entropy     | 1.5550363     |\n",
            "| policy_loss        | -0.009278492  |\n",
            "| serial_timesteps   | 640           |\n",
            "| time_elapsed       | 0.777         |\n",
            "| total_timesteps    | 640           |\n",
            "| value_loss         | 50.91967      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007967367  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.937        |\n",
            "| fps                | 1036          |\n",
            "| n_updates          | 6             |\n",
            "| policy_entropy     | 1.5262913     |\n",
            "| policy_loss        | -0.0021915943 |\n",
            "| serial_timesteps   | 768           |\n",
            "| time_elapsed       | 0.92          |\n",
            "| total_timesteps    | 768           |\n",
            "| value_loss         | 0.030670697   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00043500503  |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | -3.72          |\n",
            "| fps                | 920            |\n",
            "| n_updates          | 7              |\n",
            "| policy_entropy     | 1.505379       |\n",
            "| policy_loss        | -0.00031448936 |\n",
            "| serial_timesteps   | 896            |\n",
            "| time_elapsed       | 1.05           |\n",
            "| total_timesteps    | 896            |\n",
            "| value_loss         | 0.016023539    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000600131   |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.164        |\n",
            "| fps                | 979           |\n",
            "| n_updates          | 8             |\n",
            "| policy_entropy     | 1.4978577     |\n",
            "| policy_loss        | -0.0030078483 |\n",
            "| serial_timesteps   | 1024          |\n",
            "| time_elapsed       | 1.19          |\n",
            "| total_timesteps    | 1024          |\n",
            "| value_loss         | 0.009932226   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0029429994  |\n",
            "| clipfrac           | 0.005859375   |\n",
            "| explained_variance | -0.268        |\n",
            "| fps                | 1033          |\n",
            "| n_updates          | 9             |\n",
            "| policy_entropy     | 1.4581167     |\n",
            "| policy_loss        | -0.0143221915 |\n",
            "| serial_timesteps   | 1152          |\n",
            "| time_elapsed       | 1.32          |\n",
            "| total_timesteps    | 1152          |\n",
            "| value_loss         | 0.0059431805  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021053131  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.00203       |\n",
            "| fps                | 882           |\n",
            "| n_updates          | 10            |\n",
            "| policy_entropy     | 1.3916507     |\n",
            "| policy_loss        | -0.0037976366 |\n",
            "| serial_timesteps   | 1280          |\n",
            "| time_elapsed       | 1.44          |\n",
            "| total_timesteps    | 1280          |\n",
            "| value_loss         | 51.008724     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0005696289  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -1.33         |\n",
            "| fps                | 949           |\n",
            "| n_updates          | 11            |\n",
            "| policy_entropy     | 1.3709543     |\n",
            "| policy_loss        | -0.0016391397 |\n",
            "| serial_timesteps   | 1408          |\n",
            "| time_elapsed       | 1.59          |\n",
            "| total_timesteps    | 1408          |\n",
            "| value_loss         | 0.0070392457  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00033725004 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.000791     |\n",
            "| fps                | 1026          |\n",
            "| n_updates          | 12            |\n",
            "| policy_entropy     | 1.4007789     |\n",
            "| policy_loss        | -0.0002175034 |\n",
            "| serial_timesteps   | 1536          |\n",
            "| time_elapsed       | 1.73          |\n",
            "| total_timesteps    | 1536          |\n",
            "| value_loss         | 317.21362     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0005818701 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00299     |\n",
            "| fps                | 951          |\n",
            "| n_updates          | 13           |\n",
            "| policy_entropy     | 1.3991208    |\n",
            "| policy_loss        | -0.00455687  |\n",
            "| serial_timesteps   | 1664         |\n",
            "| time_elapsed       | 1.85         |\n",
            "| total_timesteps    | 1664         |\n",
            "| value_loss         | 319.36856    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0007450893 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | 0.00134      |\n",
            "| fps                | 954          |\n",
            "| n_updates          | 14           |\n",
            "| policy_entropy     | 1.4167843    |\n",
            "| policy_loss        | -0.003642663 |\n",
            "| serial_timesteps   | 1792         |\n",
            "| time_elapsed       | 1.99         |\n",
            "| total_timesteps    | 1792         |\n",
            "| value_loss         | 50.98384     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00063003984 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.161        |\n",
            "| fps                | 1021          |\n",
            "| n_updates          | 15            |\n",
            "| policy_entropy     | 1.4217145     |\n",
            "| policy_loss        | -0.0018343274 |\n",
            "| serial_timesteps   | 1920          |\n",
            "| time_elapsed       | 2.13          |\n",
            "| total_timesteps    | 1920          |\n",
            "| value_loss         | 0.024384502   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0010874602  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.519        |\n",
            "| fps                | 995           |\n",
            "| n_updates          | 16            |\n",
            "| policy_entropy     | 1.4436477     |\n",
            "| policy_loss        | -0.0062693376 |\n",
            "| serial_timesteps   | 2048          |\n",
            "| time_elapsed       | 2.26          |\n",
            "| total_timesteps    | 2048          |\n",
            "| value_loss         | 0.011527834   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00023862752 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.237        |\n",
            "| fps                | 915           |\n",
            "| n_updates          | 17            |\n",
            "| policy_entropy     | 1.4502741     |\n",
            "| policy_loss        | 0.0011834735  |\n",
            "| serial_timesteps   | 2176          |\n",
            "| time_elapsed       | 2.39          |\n",
            "| total_timesteps    | 2176          |\n",
            "| value_loss         | 0.008041679   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007225945  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -1.44         |\n",
            "| fps                | 931           |\n",
            "| n_updates          | 18            |\n",
            "| policy_entropy     | 1.4427909     |\n",
            "| policy_loss        | -0.0005517608 |\n",
            "| serial_timesteps   | 2304          |\n",
            "| time_elapsed       | 2.53          |\n",
            "| total_timesteps    | 2304          |\n",
            "| value_loss         | 0.0056681153  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0001727957  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.000707      |\n",
            "| fps                | 1056          |\n",
            "| n_updates          | 19            |\n",
            "| policy_entropy     | 1.4430009     |\n",
            "| policy_loss        | -0.0011089905 |\n",
            "| serial_timesteps   | 2432          |\n",
            "| time_elapsed       | 2.67          |\n",
            "| total_timesteps    | 2432          |\n",
            "| value_loss         | 45.568245     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0008528308 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | 0.0091       |\n",
            "| fps                | 1023         |\n",
            "| n_updates          | 20           |\n",
            "| policy_entropy     | 1.4245585    |\n",
            "| policy_loss        | -0.008052109 |\n",
            "| serial_timesteps   | 2560         |\n",
            "| time_elapsed       | 2.8          |\n",
            "| total_timesteps    | 2560         |\n",
            "| value_loss         | 12.71688     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00067124155 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.101        |\n",
            "| fps                | 909           |\n",
            "| n_updates          | 21            |\n",
            "| policy_entropy     | 1.3929514     |\n",
            "| policy_loss        | -0.0014258268 |\n",
            "| serial_timesteps   | 2688          |\n",
            "| time_elapsed       | 2.92          |\n",
            "| total_timesteps    | 2688          |\n",
            "| value_loss         | 0.010040017   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0007962374  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.576        |\n",
            "| fps                | 1043          |\n",
            "| n_updates          | 22            |\n",
            "| policy_entropy     | 1.3795636     |\n",
            "| policy_loss        | -0.0028628854 |\n",
            "| serial_timesteps   | 2816          |\n",
            "| time_elapsed       | 3.07          |\n",
            "| total_timesteps    | 2816          |\n",
            "| value_loss         | 0.006105851   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003983486  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.269        |\n",
            "| fps                | 1049          |\n",
            "| n_updates          | 23            |\n",
            "| policy_entropy     | 1.3739129     |\n",
            "| policy_loss        | -0.0028429762 |\n",
            "| serial_timesteps   | 2944          |\n",
            "| time_elapsed       | 3.19          |\n",
            "| total_timesteps    | 2944          |\n",
            "| value_loss         | 0.0054150377  |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0025135803 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -3.78        |\n",
            "| fps                | 1015         |\n",
            "| n_updates          | 24           |\n",
            "| policy_entropy     | 1.3349394    |\n",
            "| policy_loss        | -0.004852149 |\n",
            "| serial_timesteps   | 3072         |\n",
            "| time_elapsed       | 3.31         |\n",
            "| total_timesteps    | 3072         |\n",
            "| value_loss         | 0.0034116828 |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0014442252 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.613       |\n",
            "| fps                | 950          |\n",
            "| n_updates          | 25           |\n",
            "| policy_entropy     | 1.3041724    |\n",
            "| policy_loss        | -0.003583289 |\n",
            "| serial_timesteps   | 3200         |\n",
            "| time_elapsed       | 3.44         |\n",
            "| total_timesteps    | 3200         |\n",
            "| value_loss         | 0.0023670509 |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0005987728  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.00873       |\n",
            "| fps                | 1039          |\n",
            "| n_updates          | 26            |\n",
            "| policy_entropy     | 1.2740078     |\n",
            "| policy_loss        | -0.0018828597 |\n",
            "| serial_timesteps   | 3328          |\n",
            "| time_elapsed       | 3.58          |\n",
            "| total_timesteps    | 3328          |\n",
            "| value_loss         | 12.934711     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00042022095 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.672        |\n",
            "| fps                | 982           |\n",
            "| n_updates          | 27            |\n",
            "| policy_entropy     | 1.2507825     |\n",
            "| policy_loss        | -0.0014690863 |\n",
            "| serial_timesteps   | 3456          |\n",
            "| time_elapsed       | 3.7           |\n",
            "| total_timesteps    | 3456          |\n",
            "| value_loss         | 0.0025524758  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0009852225  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -1.13         |\n",
            "| fps                | 1027          |\n",
            "| n_updates          | 28            |\n",
            "| policy_entropy     | 1.2402315     |\n",
            "| policy_loss        | 0.00029018044 |\n",
            "| serial_timesteps   | 3584          |\n",
            "| time_elapsed       | 3.84          |\n",
            "| total_timesteps    | 3584          |\n",
            "| value_loss         | 0.0028623978  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00049778045  |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | -1.51          |\n",
            "| fps                | 1002           |\n",
            "| n_updates          | 29             |\n",
            "| policy_entropy     | 1.2170427      |\n",
            "| policy_loss        | -0.00067583064 |\n",
            "| serial_timesteps   | 3712           |\n",
            "| time_elapsed       | 3.96           |\n",
            "| total_timesteps    | 3712           |\n",
            "| value_loss         | 0.001477979    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0005035907  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -5.92         |\n",
            "| fps                | 939           |\n",
            "| n_updates          | 30            |\n",
            "| policy_entropy     | 1.2030897     |\n",
            "| policy_loss        | -0.0034865697 |\n",
            "| serial_timesteps   | 3840          |\n",
            "| time_elapsed       | 4.09          |\n",
            "| total_timesteps    | 3840          |\n",
            "| value_loss         | 0.0013275226  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000769954   |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -4.08         |\n",
            "| fps                | 1022          |\n",
            "| n_updates          | 31            |\n",
            "| policy_entropy     | 1.1842095     |\n",
            "| policy_loss        | -0.005800726  |\n",
            "| serial_timesteps   | 3968          |\n",
            "| time_elapsed       | 4.23          |\n",
            "| total_timesteps    | 3968          |\n",
            "| value_loss         | 0.00071955746 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0022515247  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.573        |\n",
            "| fps                | 910           |\n",
            "| n_updates          | 32            |\n",
            "| policy_entropy     | 1.1614053     |\n",
            "| policy_loss        | -0.009608123  |\n",
            "| serial_timesteps   | 4096          |\n",
            "| time_elapsed       | 4.35          |\n",
            "| total_timesteps    | 4096          |\n",
            "| value_loss         | 0.00052594766 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00065270485 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -2.71         |\n",
            "| fps                | 973           |\n",
            "| n_updates          | 33            |\n",
            "| policy_entropy     | 1.1134939     |\n",
            "| policy_loss        | -0.0003246545 |\n",
            "| serial_timesteps   | 4224          |\n",
            "| time_elapsed       | 4.5           |\n",
            "| total_timesteps    | 4224          |\n",
            "| value_loss         | 0.0006189164  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0008120975  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -7.24         |\n",
            "| fps                | 903           |\n",
            "| n_updates          | 34            |\n",
            "| policy_entropy     | 1.1154423     |\n",
            "| policy_loss        | -0.00219958   |\n",
            "| serial_timesteps   | 4352          |\n",
            "| time_elapsed       | 4.63          |\n",
            "| total_timesteps    | 4352          |\n",
            "| value_loss         | 0.00028168867 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00032943033 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -17.6         |\n",
            "| fps                | 946           |\n",
            "| n_updates          | 35            |\n",
            "| policy_entropy     | 1.1278418     |\n",
            "| policy_loss        | 0.00087191106 |\n",
            "| serial_timesteps   | 4480          |\n",
            "| time_elapsed       | 4.77          |\n",
            "| total_timesteps    | 4480          |\n",
            "| value_loss         | 0.0004027301  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0021201435  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -25           |\n",
            "| fps                | 1032          |\n",
            "| n_updates          | 36            |\n",
            "| policy_entropy     | 1.0925325     |\n",
            "| policy_loss        | -0.007451857  |\n",
            "| serial_timesteps   | 4608          |\n",
            "| time_elapsed       | 4.91          |\n",
            "| total_timesteps    | 4608          |\n",
            "| value_loss         | 0.00045134037 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0006091667  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -15.9         |\n",
            "| fps                | 1006          |\n",
            "| n_updates          | 37            |\n",
            "| policy_entropy     | 1.0436008     |\n",
            "| policy_loss        | -0.0011398876 |\n",
            "| serial_timesteps   | 4736          |\n",
            "| time_elapsed       | 5.04          |\n",
            "| total_timesteps    | 4736          |\n",
            "| value_loss         | 0.00051206216 |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00067918707 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -18.3         |\n",
            "| fps                | 911           |\n",
            "| n_updates          | 38            |\n",
            "| policy_entropy     | 1.048539      |\n",
            "| policy_loss        | 0.00081735593 |\n",
            "| serial_timesteps   | 4864          |\n",
            "| time_elapsed       | 5.17          |\n",
            "| total_timesteps    | 4864          |\n",
            "| value_loss         | 0.0001530233  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00036163052 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -23.7         |\n",
            "| fps                | 884           |\n",
            "| n_updates          | 39            |\n",
            "| policy_entropy     | 1.0246413     |\n",
            "| policy_loss        | -0.0029018098 |\n",
            "| serial_timesteps   | 4992          |\n",
            "| time_elapsed       | 5.31          |\n",
            "| total_timesteps    | 4992          |\n",
            "| value_loss         | 7.6606935e-05 |\n",
            "--------------------------------------\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00080090185 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.00137       |\n",
            "| fps                | 368           |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 1.6087087     |\n",
            "| policy_loss        | -0.0088572465 |\n",
            "| serial_timesteps   | 128           |\n",
            "| time_elapsed       | 1.62e-05      |\n",
            "| total_timesteps    | 128           |\n",
            "| value_loss         | 228.2729      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0010996197 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -6.09e-05    |\n",
            "| fps                | 924          |\n",
            "| n_updates          | 2            |\n",
            "| policy_entropy     | 1.6019571    |\n",
            "| policy_loss        | -0.012141735 |\n",
            "| serial_timesteps   | 256          |\n",
            "| time_elapsed       | 0.352        |\n",
            "| total_timesteps    | 256          |\n",
            "| value_loss         | 1172.3401    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.0007662161   |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | 0.00157        |\n",
            "| fps                | 1009           |\n",
            "| n_updates          | 3              |\n",
            "| policy_entropy     | 1.5893031      |\n",
            "| policy_loss        | -0.00016553327 |\n",
            "| serial_timesteps   | 384            |\n",
            "| time_elapsed       | 0.492          |\n",
            "| total_timesteps    | 384            |\n",
            "| value_loss         | 55.552906      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0003056172 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | 0.00135      |\n",
            "| fps                | 1018         |\n",
            "| n_updates          | 4            |\n",
            "| policy_entropy     | 1.5892224    |\n",
            "| policy_loss        | 0.0006240653 |\n",
            "| serial_timesteps   | 512          |\n",
            "| time_elapsed       | 0.62         |\n",
            "| total_timesteps    | 512          |\n",
            "| value_loss         | 42.526276    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00012211621 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.00538       |\n",
            "| fps                | 1021          |\n",
            "| n_updates          | 5             |\n",
            "| policy_entropy     | 1.5894645     |\n",
            "| policy_loss        | -0.0032692957 |\n",
            "| serial_timesteps   | 640           |\n",
            "| time_elapsed       | 0.747         |\n",
            "| total_timesteps    | 640           |\n",
            "| value_loss         | 43.723328     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00061064464 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.00146       |\n",
            "| fps                | 952           |\n",
            "| n_updates          | 6             |\n",
            "| policy_entropy     | 1.5948268     |\n",
            "| policy_loss        | -0.008719152  |\n",
            "| serial_timesteps   | 768           |\n",
            "| time_elapsed       | 0.874         |\n",
            "| total_timesteps    | 768           |\n",
            "| value_loss         | 88.21852      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0006456773  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00189      |\n",
            "| fps                | 983           |\n",
            "| n_updates          | 7             |\n",
            "| policy_entropy     | 1.5812974     |\n",
            "| policy_loss        | -0.0035721627 |\n",
            "| serial_timesteps   | 896           |\n",
            "| time_elapsed       | 1.01          |\n",
            "| total_timesteps    | 896           |\n",
            "| value_loss         | 279.82858     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00028596    |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00416      |\n",
            "| fps                | 1045          |\n",
            "| n_updates          | 8             |\n",
            "| policy_entropy     | 1.572342      |\n",
            "| policy_loss        | -0.0032718321 |\n",
            "| serial_timesteps   | 1024          |\n",
            "| time_elapsed       | 1.14          |\n",
            "| total_timesteps    | 1024          |\n",
            "| value_loss         | 306.8284      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00048394708 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00328      |\n",
            "| fps                | 1002          |\n",
            "| n_updates          | 9             |\n",
            "| policy_entropy     | 1.5656965     |\n",
            "| policy_loss        | 0.0006243293  |\n",
            "| serial_timesteps   | 1152          |\n",
            "| time_elapsed       | 1.27          |\n",
            "| total_timesteps    | 1152          |\n",
            "| value_loss         | 34.835464     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0002048725 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00309     |\n",
            "| fps                | 977          |\n",
            "| n_updates          | 10           |\n",
            "| policy_entropy     | 1.5680015    |\n",
            "| policy_loss        | -0.003770605 |\n",
            "| serial_timesteps   | 1280         |\n",
            "| time_elapsed       | 1.4          |\n",
            "| total_timesteps    | 1280         |\n",
            "| value_loss         | 27.568062    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00034993284 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.0031       |\n",
            "| fps                | 1050          |\n",
            "| n_updates          | 11            |\n",
            "| policy_entropy     | 1.5679176     |\n",
            "| policy_loss        | -0.0047710314 |\n",
            "| serial_timesteps   | 1408          |\n",
            "| time_elapsed       | 1.53          |\n",
            "| total_timesteps    | 1408          |\n",
            "| value_loss         | 165.66718     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00027127122 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.00501       |\n",
            "| fps                | 1026          |\n",
            "| n_updates          | 12            |\n",
            "| policy_entropy     | 1.5690895     |\n",
            "| policy_loss        | 0.0025590921  |\n",
            "| serial_timesteps   | 1536          |\n",
            "| time_elapsed       | 1.65          |\n",
            "| total_timesteps    | 1536          |\n",
            "| value_loss         | 11.624006     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00042452817 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00169      |\n",
            "| fps                | 899           |\n",
            "| n_updates          | 13            |\n",
            "| policy_entropy     | 1.5788213     |\n",
            "| policy_loss        | 0.0011034759  |\n",
            "| serial_timesteps   | 1664          |\n",
            "| time_elapsed       | 1.78          |\n",
            "| total_timesteps    | 1664          |\n",
            "| value_loss         | 75.433586     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 7.944749e-05  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00157      |\n",
            "| fps                | 1012          |\n",
            "| n_updates          | 14            |\n",
            "| policy_entropy     | 1.5679545     |\n",
            "| policy_loss        | -0.0007391976 |\n",
            "| serial_timesteps   | 1792          |\n",
            "| time_elapsed       | 1.92          |\n",
            "| total_timesteps    | 1792          |\n",
            "| value_loss         | 123.66882     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 4.744753e-05 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00605     |\n",
            "| fps                | 959          |\n",
            "| n_updates          | 15           |\n",
            "| policy_entropy     | 1.5661912    |\n",
            "| policy_loss        | 0.0006966529 |\n",
            "| serial_timesteps   | 1920         |\n",
            "| time_elapsed       | 2.05         |\n",
            "| total_timesteps    | 1920         |\n",
            "| value_loss         | 53.162674    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00025308738 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00166      |\n",
            "| fps                | 899           |\n",
            "| n_updates          | 16            |\n",
            "| policy_entropy     | 1.5602269     |\n",
            "| policy_loss        | 2.4425506e-05 |\n",
            "| serial_timesteps   | 2048          |\n",
            "| time_elapsed       | 2.19          |\n",
            "| total_timesteps    | 2048          |\n",
            "| value_loss         | 203.86328     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00029113897 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00324      |\n",
            "| fps                | 1011          |\n",
            "| n_updates          | 17            |\n",
            "| policy_entropy     | 1.5699614     |\n",
            "| policy_loss        | 0.003941532   |\n",
            "| serial_timesteps   | 2176          |\n",
            "| time_elapsed       | 2.33          |\n",
            "| total_timesteps    | 2176          |\n",
            "| value_loss         | 14.266685     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0032306088 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.0123      |\n",
            "| fps                | 958          |\n",
            "| n_updates          | 18           |\n",
            "| policy_entropy     | 1.5915759    |\n",
            "| policy_loss        | -0.015563331 |\n",
            "| serial_timesteps   | 2304         |\n",
            "| time_elapsed       | 2.46         |\n",
            "| total_timesteps    | 2304         |\n",
            "| value_loss         | 16.979076    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0014787926 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.004       |\n",
            "| fps                | 1011         |\n",
            "| n_updates          | 19           |\n",
            "| policy_entropy     | 1.5962746    |\n",
            "| policy_loss        | 0.013229037  |\n",
            "| serial_timesteps   | 2432         |\n",
            "| time_elapsed       | 2.59         |\n",
            "| total_timesteps    | 2432         |\n",
            "| value_loss         | 365.61133    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00019859178 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.000764     |\n",
            "| fps                | 956           |\n",
            "| n_updates          | 20            |\n",
            "| policy_entropy     | 1.5801228     |\n",
            "| policy_loss        | 7.130136e-05  |\n",
            "| serial_timesteps   | 2560          |\n",
            "| time_elapsed       | 2.72          |\n",
            "| total_timesteps    | 2560          |\n",
            "| value_loss         | 629.658       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0002014304 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00577     |\n",
            "| fps                | 1037         |\n",
            "| n_updates          | 21           |\n",
            "| policy_entropy     | 1.5685871    |\n",
            "| policy_loss        | -0.003032111 |\n",
            "| serial_timesteps   | 2688         |\n",
            "| time_elapsed       | 2.86         |\n",
            "| total_timesteps    | 2688         |\n",
            "| value_loss         | 217.78874    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00017111229 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.000485      |\n",
            "| fps                | 1011          |\n",
            "| n_updates          | 22            |\n",
            "| policy_entropy     | 1.58303       |\n",
            "| policy_loss        | 9.7349286e-05 |\n",
            "| serial_timesteps   | 2816          |\n",
            "| time_elapsed       | 2.98          |\n",
            "| total_timesteps    | 2816          |\n",
            "| value_loss         | 64.69249      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0002609563  |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00579      |\n",
            "| fps                | 955           |\n",
            "| n_updates          | 23            |\n",
            "| policy_entropy     | 1.5913466     |\n",
            "| policy_loss        | -0.0010790649 |\n",
            "| serial_timesteps   | 2944          |\n",
            "| time_elapsed       | 3.12          |\n",
            "| total_timesteps    | 2944          |\n",
            "| value_loss         | 81.98844      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00043118157 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00663      |\n",
            "| fps                | 964           |\n",
            "| n_updates          | 24            |\n",
            "| policy_entropy     | 1.5901335     |\n",
            "| policy_loss        | -0.0048084166 |\n",
            "| serial_timesteps   | 3072          |\n",
            "| time_elapsed       | 3.25          |\n",
            "| total_timesteps    | 3072          |\n",
            "| value_loss         | 95.075        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00048820852 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00236      |\n",
            "| fps                | 939           |\n",
            "| n_updates          | 25            |\n",
            "| policy_entropy     | 1.5642606     |\n",
            "| policy_loss        | -0.0059979158 |\n",
            "| serial_timesteps   | 3200          |\n",
            "| time_elapsed       | 3.39          |\n",
            "| total_timesteps    | 3200          |\n",
            "| value_loss         | 61.92583      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0002503645 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00413     |\n",
            "| fps                | 986          |\n",
            "| n_updates          | 26           |\n",
            "| policy_entropy     | 1.5653685    |\n",
            "| policy_loss        | 0.0013241107 |\n",
            "| serial_timesteps   | 3328         |\n",
            "| time_elapsed       | 3.52         |\n",
            "| total_timesteps    | 3328         |\n",
            "| value_loss         | 4.4551134    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00085202313 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00488      |\n",
            "| fps                | 999           |\n",
            "| n_updates          | 27            |\n",
            "| policy_entropy     | 1.5729233     |\n",
            "| policy_loss        | -0.00792479   |\n",
            "| serial_timesteps   | 3456          |\n",
            "| time_elapsed       | 3.66          |\n",
            "| total_timesteps    | 3456          |\n",
            "| value_loss         | 15.260354     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00028042658 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00246      |\n",
            "| fps                | 925           |\n",
            "| n_updates          | 28            |\n",
            "| policy_entropy     | 1.5842887     |\n",
            "| policy_loss        | -0.005530799  |\n",
            "| serial_timesteps   | 3584          |\n",
            "| time_elapsed       | 3.79          |\n",
            "| total_timesteps    | 3584          |\n",
            "| value_loss         | 12.717931     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0009898003 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | 0.0029       |\n",
            "| fps                | 892          |\n",
            "| n_updates          | 29           |\n",
            "| policy_entropy     | 1.5753087    |\n",
            "| policy_loss        | 9.443262e-06 |\n",
            "| serial_timesteps   | 3712         |\n",
            "| time_elapsed       | 3.93         |\n",
            "| total_timesteps    | 3712         |\n",
            "| value_loss         | 16.35068     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00037634932 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00744      |\n",
            "| fps                | 1036          |\n",
            "| n_updates          | 30            |\n",
            "| policy_entropy     | 1.5392163     |\n",
            "| policy_loss        | -0.003694545  |\n",
            "| serial_timesteps   | 3840          |\n",
            "| time_elapsed       | 4.07          |\n",
            "| total_timesteps    | 3840          |\n",
            "| value_loss         | 5.540182      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00038258336 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.000608      |\n",
            "| fps                | 985           |\n",
            "| n_updates          | 31            |\n",
            "| policy_entropy     | 1.5513679     |\n",
            "| policy_loss        | -0.0033671514 |\n",
            "| serial_timesteps   | 3968          |\n",
            "| time_elapsed       | 4.2           |\n",
            "| total_timesteps    | 3968          |\n",
            "| value_loss         | 32.933952     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00017001673 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00164      |\n",
            "| fps                | 1021          |\n",
            "| n_updates          | 32            |\n",
            "| policy_entropy     | 1.550097      |\n",
            "| policy_loss        | -0.003113218  |\n",
            "| serial_timesteps   | 4096          |\n",
            "| time_elapsed       | 4.33          |\n",
            "| total_timesteps    | 4096          |\n",
            "| value_loss         | 19.330425     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.75587e-05    |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | 0.0116         |\n",
            "| fps                | 954            |\n",
            "| n_updates          | 33             |\n",
            "| policy_entropy     | 1.544605       |\n",
            "| policy_loss        | -0.00014577527 |\n",
            "| serial_timesteps   | 4224           |\n",
            "| time_elapsed       | 4.46           |\n",
            "| total_timesteps    | 4224           |\n",
            "| value_loss         | 1.1666075      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000339811   |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.0015       |\n",
            "| fps                | 915           |\n",
            "| n_updates          | 34            |\n",
            "| policy_entropy     | 1.5802066     |\n",
            "| policy_loss        | -0.0074157296 |\n",
            "| serial_timesteps   | 4352          |\n",
            "| time_elapsed       | 4.59          |\n",
            "| total_timesteps    | 4352          |\n",
            "| value_loss         | 11.095011     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0034928143 |\n",
            "| clipfrac           | 0.033203125  |\n",
            "| explained_variance | -0.00105     |\n",
            "| fps                | 950          |\n",
            "| n_updates          | 35           |\n",
            "| policy_entropy     | 1.5740288    |\n",
            "| policy_loss        | -0.016949546 |\n",
            "| serial_timesteps   | 4480         |\n",
            "| time_elapsed       | 4.73         |\n",
            "| total_timesteps    | 4480         |\n",
            "| value_loss         | 1.5623361    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0012547807 |\n",
            "| clipfrac           | 0.0          |\n",
            "| explained_variance | -0.00174     |\n",
            "| fps                | 917          |\n",
            "| n_updates          | 36           |\n",
            "| policy_entropy     | 1.5843754    |\n",
            "| policy_loss        | 0.0035553847 |\n",
            "| serial_timesteps   | 4608         |\n",
            "| time_elapsed       | 4.87         |\n",
            "| total_timesteps    | 4608         |\n",
            "| value_loss         | 28.445585    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.000430764   |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | -0.00042      |\n",
            "| fps                | 1044          |\n",
            "| n_updates          | 37            |\n",
            "| policy_entropy     | 1.5652722     |\n",
            "| policy_loss        | -0.0051329075 |\n",
            "| serial_timesteps   | 4736          |\n",
            "| time_elapsed       | 5.01          |\n",
            "| total_timesteps    | 4736          |\n",
            "| value_loss         | 512.63794     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00039050492 |\n",
            "| clipfrac           | 0.0           |\n",
            "| explained_variance | 0.000936      |\n",
            "| fps                | 960           |\n",
            "| n_updates          | 38            |\n",
            "| policy_entropy     | 1.5609998     |\n",
            "| policy_loss        | -0.0047615077 |\n",
            "| serial_timesteps   | 4864          |\n",
            "| time_elapsed       | 5.14          |\n",
            "| total_timesteps    | 4864          |\n",
            "| value_loss         | 245.58977     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00016744807  |\n",
            "| clipfrac           | 0.0            |\n",
            "| explained_variance | -0.000746      |\n",
            "| fps                | 1018           |\n",
            "| n_updates          | 39             |\n",
            "| policy_entropy     | 1.5660927      |\n",
            "| policy_loss        | -0.00018606335 |\n",
            "| serial_timesteps   | 4992           |\n",
            "| time_elapsed       | 5.27           |\n",
            "| total_timesteps    | 4992           |\n",
            "| value_loss         | 98.774574      |\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR5rdvTK2hGN",
        "outputId": "20cc6f1d-2ce1-4380-e087-9e6f11da0956"
      },
      "source": [
        "##VALIDATION \n",
        "obs1 = env1.reset()\n",
        "obs2=env2.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action_a, _ = model1.predict(obs1, deterministic=True)\n",
        "  action_b,_ = model2.predict(obs2, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action:a\", action_a)\n",
        "  print(\"Action:b\",action_b)\n",
        "  obs1, reward1, done, info = env1.step(action_a)\n",
        "  obs2, reward2, done, info = env2.step(action_b)\n",
        "  print('obs a =', obs1, 'reward a =', reward1, 'done=', done)\n",
        "  print('obs b =', obs2, 'reward b =', reward2, 'done=', done)\n",
        "  env1.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward_a=\", reward1)\n",
        "    print(\"Goal reached!\",\"reward_b=\",reward2)\n",
        "    break"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 3. 7. 4. 1.] reward a = 0.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 2\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 2. 7. 4. 1.] reward a = 0.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 3\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 1. 7. 4. 1.] reward a = 0.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 4\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 5\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 6\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 7\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 8\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 9\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 10\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 11\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 12\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 13\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 14\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 15\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 16\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 17\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 18\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 19\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n",
            "Step 20\n",
            "Action:a 3\n",
            "Action:b 1\n",
            "obs a = [0. 0. 7. 4. 1.] reward a = -1.0 done= False\n",
            "obs b = [4. 2. 4. 3. 0.] reward b = -20.0 done= False\n"
          ]
        }
      ]
    }
  ]
}